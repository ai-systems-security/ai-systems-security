## Hi there üëã

<!--
**ai-systems-security/ai-systems-security** is a ‚ú® _special_ ‚ú® repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- üî≠ I‚Äôm currently working on ...
- üå± I‚Äôm currently learning ...
- üëØ I‚Äôm looking to collaborate on ...
- ü§î I‚Äôm looking for help with ...
- üí¨ Ask me about ...
- üì´ How to reach me: ...
- üòÑ Pronouns: ...
- ‚ö° Fun fact: ...
-->

# AI Systems Security Experiments üõ°Ô∏èü§ñ

[![Profile Views](https://komarev.com/ghpvc/?username=ai-systems-security&color=blue)](https://github.com/ai-systems-security)  [![GitHub followers](https://img.shields.io/github/followers/ai-systems-security?label=Follow&style=social)](https://github.com/ai-systems-security?tab=followers)  [![GitHub stars](https://img.shields.io/github/stars/ai-systems-security?style=social)](https://github.com/ai-systems-security)

Welcome to my **AI Systems Security Experiments** profile! This repository collection explores **security risks in AI Agents** and **adversarial machine learning in financial AI systems**.  Hands-on projects demonstrate vulnerabilities in multi-agent systems, unsafe AI operations, and adversarial attacks on machine learning models.

---

## üöÄ Highlights

### Attacking Insecure Agents
- Excessive Database Agency: LLMs issuing unrestricted queries like `MATCH (n) RETURN n` on Neo4j.  
- Multi-Agent Manipulation: Prompt-engineered attacks influencing collaborative AI agents.  
- Prompt Injection & Policy Bypass: Tricks agents into leaking sensitive information.  
- Unsafe Tool Execution: Exploits AI agents with unrestricted tool access.

### Financial AI Security
- Evasion attacks on credit scoring models (tabular data).  
- Black-box and white-box attacks using ART and Foolbox.  
- Future roadmap includes model extraction, inference attacks, poisoning, and defenses.  

### üéØ Goals
- Simulate real-world attacks on AI agents and financial AI systems.  
- Explore mitigations: guardrails, whitelisting, monitoring, robust training.  
- Provide a hands-on learning environment for AI security research.

---

### ‚öíÔ∏è Tools & Tech 

**AI/ML & Security Frameworks**  

![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python&logoColor=white)  ![PyTorch](https://img.shields.io/badge/PyTorch-black?logo=pytorch&logoColor=orange)  ![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-F7931E?logo=scikit-learn&logoColor=white)  ![XGBoost](https://img.shields.io/badge/XGBoost-0C4A6E?logo=apache&logoColor=white)  ![Adversarial Robustness Toolbox](https://img.shields.io/badge/Adversarial%20Robustness%20Toolbox-ART-green?logo=ibm&logoColor=white)  ![TextAttack](https://img.shields.io/badge/TextAttack-red?logo=openai&logoColor=white)  ![PromptBench](https://img.shields.io/badge/PromptBench-purple?logo=github&logoColor=white)  

**Databases & Graphs** 

![Neo4j](https://img.shields.io/badge/Neo4j-008CC1?logo=neo4j&logoColor=white)  ![PostgreSQL](https://img.shields.io/badge/PostgreSQL-336791?logo=postgresql&logoColor=white)  

**LLM & Agents**  

![LangChain](https://img.shields.io/badge/LangChain-black?logo=chainlink&logoColor=blue)  ![LangGraph](https://img.shields.io/badge/LangGraph-orange?logo=python&logoColor=white)  ![Ollama](https://img.shields.io/badge/Ollama-000000?logo=ollama&logoColor=white)  ![Llama3.1](https://img.shields.io/badge/Llama-gray?logo=ai&logoColor=white)  

**Security Standards & Regulatory Compliance**  
![OWASP LLM Top 10](https://img.shields.io/badge/OWASP-LLM%20Top--10-red?logo=owasp&logoColor=white)  ![OWASP Machine Learning Security Top 10](https://img.shields.io/badge/OWASP-ML%20Top--10-purple?logo=owasp&logoColor=white)  ![OWASP Agentic AI Threats](https://img.shields.io/badge/OWASP-Agentic--AI--Threats-orange?logo=owasp&logoColor=white)  ![GDPR AI](https://img.shields.io/badge/GDPR-AI-blue?logo=gdpr&logoColor=white)  ![ISO/IEC 42001:2023](https://img.shields.io/badge/ISO--IEC-42001:2023-green?logo=iso&logoColor=white)  ![NIST AI RMF](https://img.shields.io/badge/NIST-AI%20RMF-blue?logo=gov&logoColor=white)  ![MITRE ATLAS](https://img.shields.io/badge/MITRE-ATLAS-red?logo=mitre&logoColor=white)

### Running Experiments
- Agent experiments: `jupyter notebook: excessive_db_agency1.ipynb, excessive_db_agency2.ipynb`  
- Adversarial ML on credit scoring: `jupyter notebook: bim_attack-art-logistic_regression.ipynb, cw_attack-art-logistic_regression.ipynb, deepfool_attack-art-logistic_regression.ipynb, ead_attack-art-logistic_regression.ipynb, fgm_attack-art-logistic_regression.ipynb, jsma_attack-art-logistic_regression.ipynb, pgd_attack-art-logistic_regression.ipynb, boundary _attack(targeted)_attack-art-logistic_regression.ipynb, boundary _attack(untargeted)_attack-art-logistic_regression.ipynb, hsj_attack-art-logistic_regression.ipynb, zoo_attack-art-logistic_regression.ipynb`

---

## üìä GitHub Stats
![ai-systems-security's GitHub stats](https://github-readme-stats.vercel.app/api?username=ai-systems-security&show_icons=true&hide_title=true&count_private=true&theme=radical)

![Top Languages](https://github-readme-stats.vercel.app/api/top-langs/?username=ai-systems-security&layout=compact&theme=radical)

---

## ‚ö†Ô∏è Security Notes
- For **educational purposes only**.  
- Do **not deploy vulnerable agents** or ML models in production.  
- Always enforce: whitelisting, tool guardrails, interaction monitoring, policy & prompt security, adversarially robust ML defenses.

---

## üìö References
- [OWASP Top 10 for LLM Applications](https://owasp.org/www-project-top-10-for-llm-applications/)  
- [OWASP ML Security Top 10 (2023)](https://owasp.org/www-project-machine-learning-security-top-10/)  
- [LangGraph Documentation](https://python.langchain.com/docs/langgraph)  
- [Neo4j Graph Database](https://neo4j.com/)  
- [Adversarial Robustness Toolbox (ART)](https://github.com/Trusted-AI/adversarial-robustness-toolbox)  
- [Agentic AI research papers & security advisories]  

---

Thanks for visiting my profile! üëã

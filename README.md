# AI Systems Security Experiments üõ°Ô∏èü§ñ

[![Profile Views](https://komarev.com/ghpvc/?username=ai-systems-security&color=blue)](https://github.com/ai-systems-security)  [![GitHub stars](https://img.shields.io/github/stars/ai-systems-security?style=social&cacheSeconds=60)](https://github.com/ai-systems-security)  [![GitHub followers](https://img.shields.io/github/followers/ai-systems-security?label=Follow&style=social&cacheSeconds=60)](https://github.com/ai-systems-security?tab=followers)

Welcome to my **AI Systems Security Experiments** profile! This repository collection explores **security risks in AI Agents** and **adversarial machine learning in financial AI systems**.  Hands-on projects demonstrate vulnerabilities in multi-agent systems, unsafe AI operations, and adversarial attacks on machine learning models.

---

## üöÄ Highlights

### Attacking Insecure Agents
- Excessive Database Agency: LLMs issuing unrestricted queries like `MATCH (n) RETURN n` on Neo4j.  
- Multi-Agent Manipulation: Prompt-engineered attacks influencing collaborative AI agents.  
- Prompt Injection & Policy Bypass: Tricks agents into leaking sensitive information.  
- Unsafe Tool Execution: Exploits AI agents with unrestricted tool access.

### Financial AI Security
- Evasion attacks on credit scoring models (tabular data).  
- Black-box and white-box attacks using ART and Foolbox.  
- Future roadmap includes model extraction, inference attacks, poisoning, and defenses.  

### üéØ Goals
- Simulate real-world attacks on AI agents and financial AI systems.  
- Explore mitigations: guardrails, whitelisting, monitoring, robust training.  
- Provide a hands-on learning environment for AI security research.

---

### ‚öíÔ∏è Tools & Tech 

**AI/ML & Security Frameworks**  

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python&logoColor=white)](https://www.python.org/)  [![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?logo=tensorflow&logoColor=white)](https://www.tensorflow.org/)  [![PyTorch](https://img.shields.io/badge/PyTorch-black?logo=pytorch&logoColor=orange)](https://pytorch.org/)  [![ART](https://img.shields.io/badge/ART-Adversarial%20Robustness%20Toolbox-green?logo=ibm&logoColor=white)](https://adversarial-robustness-toolbox.readthedocs.io/)  [![Foolbox](https://img.shields.io/badge/Foolbox-red?logo=github&logoColor=white)](https://foolbox.readthedocs.io/)  [![SecML](https://img.shields.io/badge/SecML-purple?logo=github&logoColor=white)](https://secml.readthedocs.io/en/v0.15/)  [![PrivacyRaven](https://img.shields.io/badge/PrivacyRaven-gray?logo=github&logoColor=white)](https://github.com/trailofbits/PrivacyRaven)  [![TextAttack](https://img.shields.io/badge/TextAttack-red?logo=openai&logoColor=white)](https://textattack.readthedocs.io/en/master/)  [![PromptBench](https://img.shields.io/badge/PromptBench-purple?logo=github&logoColor=white)](https://github.com/microsoft/promptbench)  [![LlamaFirewall](https://img.shields.io/badge/LlamaFirewall-FF4500?logo=github&logoColor=white)](https://github.com/YourRepo/LlamaFirewall)

**Databases & Graphs**  

[![Neo4j](https://img.shields.io/badge/Neo4j-008CC1?logo=neo4j&logoColor=white)](https://neo4j.com/)  [![PostgreSQL](https://img.shields.io/badge/PostgreSQL-336791?logo=postgresql&logoColor=white)](https://www.postgresql.org/)  

**LLM & Agents**  

[![LangChain](https://img.shields.io/badge/LangChain-black?logo=chainlink&logoColor=blue)](https://www.langchain.com/)  [![LangGraph](https://img.shields.io/badge/LangGraph-orange?logo=python&logoColor=white)](https://github.com/langgraph/langgraph)  [![Ollama](https://img.shields.io/badge/Ollama-000000?logo=github&logoColor=white)](https://ollama.com/)  [![LLaMA](https://img.shields.io/badge/LLaMA-FF4500?logo=meta&logoColor=white)](https://ai.meta.com/llama/)

**Security Standards & Regulatory Compliance**  

[![OWASP LLM Top 10](https://img.shields.io/badge/OWASP-LLM%20Top--10-red?logo=owasp&logoColor=white)](https://owasp.org/www-project-top-10-for-large-language-model-applications/)  [![OWASP ML Top 10](https://img.shields.io/badge/OWASP-ML%20Top--10-purple?logo=owasp&logoColor=white)](https://owasp.org/www-project-machine-learning-security-top-10/)  [![OWASP Agentic AI Threats](https://img.shields.io/badge/OWASP-Agentic--AI--Threats-orange?logo=owasp&logoColor=white)](https://genai.owasp.org/resource/agentic-ai-threats-and-mitigations/)  [![NIST AI RMF](https://img.shields.io/badge/NIST-AI%20RMF-blue?logo=gov&logoColor=white)](https://www.nist.gov/itl/ai-risk-management-framework)  [![MITRE ATLAS](https://img.shields.io/badge/MITRE-ATLAS-red?logo=mitre&logoColor=white)](https://atlas.mitre.org/)  [![GDPR AI](https://img.shields.io/badge/GDPR-AI-blue?logo=gdpr&logoColor=white)](https://gdpr-info.eu/)  [![ISO/IEC 42001:2023](https://img.shields.io/badge/ISO--IEC%2042001:2023-green?logo=iso&logoColor=white)](https://www.iso.org/standard/42001)  

### Running Experiments
- **Agent experiments**: `jupyter notebook: excessive_db_agency1.ipynb, excessive_db_agency2.ipynb`  
- **Adversarial ML on credit scoring**: `jupyter notebook: bim_attack-art-logistic_regression.ipynb, cw_attack-art-logistic_regression.ipynb, deepfool_attack-art-logistic_regression.ipynb, ead_attack-art-logistic_regression.ipynb, fgm_attack-art-logistic_regression.ipynb, jsma_attack-art-logistic_regression.ipynb, pgd_attack-art-logistic_regression.ipynb, boundary _attack(targeted)_attack-art-logistic_regression.ipynb, boundary _attack(untargeted)_attack-art-logistic_regression.ipynb, hsj_attack-art-logistic_regression.ipynb, zoo_attack-art-logistic_regression.ipynb`

---

## üë®‚Äçüíª About Me

- üë®‚Äçüíª I am an **AI Security Engineer** with a Masters Degree in **Cybersecurity** and **AI** specialized in agentic AI security, multi-agent system vulnerabilities, and adversarial machine learning.  
- üî≠ I‚Äôm currently working on **attacking and securing agentic AI systems**, including multi-agent frameworks, LangGraph experiments, and adversarial ML in financial AI.  
- üëØ I‚Äôm looking to collaborate on **open-source AI security tools, threat simulations, and RAG-based LLM safety experiments**.  
- ü§î I‚Äôm looking for help with **developing robust multi-agent security policies and automated monitoring for agentic AI**.  
- üí¨ Ask me about **LLM prompt injection, multi-agent security attacks, adversarial ML in finance, and ethical AI deployment**.  
- üì´ How to reach me: **[beteaberra@gmail.com]**  
- ‚ö° Fun fact: **I'm tying to run experiments where AI agents try to hack themselves‚Ä¶ safely, of course üòâ**

---

## üìä GitHub Stats
![ai-systems-security's GitHub stats](https://github-readme-stats.vercel.app/api?username=ai-systems-security&show_icons=true&hide_title=true&count_private=true&theme=radical&cache_seconds=60)

![Top Languages](https://github-readme-stats.vercel.app/api/top-langs/?username=ai-systems-security&layout=compact&theme=radical&cache_seconds=60)

---

## ‚ö†Ô∏è Security Notes
- For **educational purposes only**.  
- Do **not deploy vulnerable agents** or ML models in production.  
- Always enforce: whitelisting, tool guardrails, interaction monitoring, policy & prompt security, adversarially robust ML defenses.

---

## üìö References
- **OWASP: [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/)**, **[OWASP Machine Learning Security Top 10](https://owasp.org/www-project-machine-learning-security-top-10/)**, **[OWASP Agentic AI Threats & Mitigations](https://genai.owasp.org/resource/agentic-ai-threats-and-mitigations/)**  
- **AI Governance, Compliance, Frameworks: [ISO/IEC 42001:2023](https://www.iso.org/standard/42001)**, **[NIST AI RMF](https://www.nist.gov/itl/ai-risk-management-framework)**, **[MITRE ATLAS](https://atlas.mitre.org/)**, **[GDPR AI](https://gdpr-info.eu/)**
- **[LangGraph Documentation](https://python.langchain.com/docs/langgraph)**
- **[Neo4j Graph Database](https://neo4j.com/)**  
- **Adversarial Attacks**:  **[Fast Gradient Method (FGM)](https://arxiv.org/abs/1412.6572)**, **[Basic Iterative Method (BIM)](https://arxiv.org/abs/1607.02533)**, **[Projected Gradient Descent (PGD)](https://arxiv.org/abs/1706.06083)**, **[Carlini & Wagner (C&W)](https://arxiv.org/abs/1608.04644)**, **[ElasticNet Attack (EAD)](https://arxiv.org/abs/1709.04114)**, **[Jacobian-based Saliency Map Attack (JSMA)](https://arxiv.org/abs/1511.07528)**, **[DeepFool](https://arxiv.org/abs/1511.04599)**, **[Zeroth Order Optimization (ZOO)](https://arxiv.org/abs/1708.03999)**, **[HopSkipJump (HSJ)](https://arxiv.org/abs/1904.02144)**, **[Boundary Attack](https://arxiv.org/abs/1712.04248)** 
- Agentic AI Research Papers
  
---

Thanks for visiting my profile! üëã
